{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from hashlib import md5\n",
    "from pathlib import Path\n",
    "from operator import methodcaller\n",
    "from itertools import chain\n",
    "import datetime as dt\n",
    "from functools import partial\n",
    "from pprint import pprint  #as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import attr\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from toolz import curry\n",
    "import pandas as pd\n",
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binascii\n",
    "import hashlib\n",
    "def hash_utf8(string):\n",
    "    \"\"\"given utf8 string return md5 hash value as hex string\"\"\"\n",
    "    hasher = hashlib.md5()\n",
    "    hasher.update(string.encode(\"utf-8\"))\n",
    "    return binascii.hexlify(hasher.digest()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as log\n",
    "log.disable(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster\n",
    "cluster = LocalCluster(processes=False)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client('127.0.0.1:36599')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = len(client.ncores())\n",
    "n_cores = sum(client.ncores().values())\n",
    "print(n_workers, n_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bllb_path = str(Path(r\"../../../code/python/bllb\").resolve())\n",
    "sys.path.insert(0, bllb_path)\n",
    "from bllb_logging import *\n",
    "from bllb import ppiter  #, hash_utf8\n",
    "\n",
    "LOG_ON = False\n",
    "LOG_LEVEL = \"WARNING\"  #\"DEBUG\"\n",
    "def start_log(enable=True, lvl='WARNING', std_lib=True):\n",
    "    log = setup_logging(enable, lvl, std_lib=std_lib)\n",
    "    log.info('examinator logging started')\n",
    "    return log\n",
    "log_on = LOG_ON\n",
    "log_level = LOG_LEVEL\n",
    "log = start_log(log_on, log_level, std_lib=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def md5_blocks(path, blocksize=1024 * 2048) -> str:\n",
    "    path = Path(path)\n",
    "    if not path.is_dir():\n",
    "        try:\n",
    "            hasher = md5()\n",
    "            with path.open('rb') as file:\n",
    "                block = file.read(blocksize)\n",
    "                while len(block) > 0:\n",
    "                    hasher.update(block)\n",
    "                    block = file.read(blocksize)\n",
    "            return hasher.hexdigest()\n",
    "        except Exception as error:\n",
    "            log.warning(\n",
    "                f'Error trying to hash item: {str(path)}\\nError:\\n{error}')\n",
    "            return\n",
    "    else:\n",
    "        dbg(f'Item is a directory and will not be hashed.  {str(path)}')\n",
    "        return\n",
    "def glob_paths(path):\n",
    "    try:\n",
    "        path = Path(path)\n",
    "        if path.is_dir():\n",
    "            return path.rglob('*')\n",
    "        else:\n",
    "            return path\n",
    "    except Exception as error:\n",
    "        log.warning(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat(path, opt_md5=True, opt_pid=False) -> dict:\n",
    "    log.debug(path)\n",
    "    try:\n",
    "        path = Path(path)\n",
    "        info = dict([\n",
    "            _ for _ in inspect.getmembers(path.lstat())\n",
    "            if not _[0].startswith('_') and not inspect.isbuiltin(_[1])\n",
    "        ])\n",
    "        info.update(\n",
    "            dict([\n",
    "                (_[0], str(_[1])) for _ in inspect.getmembers(path)\n",
    "                if '__' not in _[0] and '<' not in str(_[1])\n",
    "            ]))\n",
    "        info.update(\n",
    "            dict([(str(_[0]), methodcaller(_[0])(path))\n",
    "                  for _ in inspect.getmembers(path)\n",
    "                  if _[0].startswith('is_') and _[0] != 'is_mount']))\n",
    "        info['path'] = str(path)\n",
    "        info['path_hash'] = hash_utf8(str(path))\n",
    "        info['f_atime'] = dt.datetime.fromtimestamp(info['st_atime'])\n",
    "        info['f_ctime'] = dt.datetime.fromtimestamp(info['st_ctime'])\n",
    "        info['f_mtime'] = dt.datetime.fromtimestamp(info['st_mtime'])\n",
    "        if opt_md5:\n",
    "            if not path.is_dir():\n",
    "                try:\n",
    "                    md5_hash = md5_blocks(path)\n",
    "                    info['md5'] = md5_hash\n",
    "                except:\n",
    "                    log.warning(f'Could not hash item: {str(path)}')\n",
    "            else:\n",
    "                log.debug(f'Item is a directory and will not be hashed.  {str(path)}'\n",
    "                    )\n",
    "        if opt_pid:\n",
    "            log.debug(f\"working using OS pid: {os.getpid()}, opt_pid: {opt_pid}\")\n",
    "        return info\n",
    "    except Exception as error:\n",
    "        log.warning(error)\n",
    "        return {'path': str(path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('.')\n",
    "get_stat(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lists):\n",
    "    return reduce(lambda res, x: res + (flatten(x) if isinstance(x, list) else [x]), lists, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "basepaths = ['..']\n",
    "opt_md5=False\n",
    "#def proc_paths(basepaths, opt_md5=True):\n",
    "\"\"\"proc_paths uses Dask client to map path_stat over basepaths.\"\"\"\n",
    "paths = chain.from_iterable(map(glob_paths, basepaths))\n",
    "pstat = partial(get_stat, opt_md5=opt_md5, opt_pid=True)\n",
    "results = client.map(pstat, paths)\n",
    "data = [_.result() for _ in results]\n",
    "ddf = dd.from_pandas(pd.DataFrame(data), npartitions=4)\n",
    "df = ddf.compute()\n",
    "#df['idx'] = df.index\n",
    "#df['path_hash'] = df.path.map(str).map(hash_utf8)\n",
    "#times = df.loc[:, ['idx', 'path', 'f_ctime', 'f_mtime', 'f_atime']].melt(id_vars=['idx', 'path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('..')\n",
    "basepaths = [str(path)]\n",
    "def proc_item(path):\n",
    "    return [*map(str, path.iterdir())] + [*map(proc_item, filter(Path.is_dir, path.iterdir()))]\n",
    "result = proc_item(path)\n",
    "print(len([*path.rglob('*')]))\n",
    "print(len(result))\n",
    "results = [*result]\n",
    "print(len(results))\n",
    "final = flatten(results)\n",
    "print(len(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lists):\n",
    "    return reduce(lambda res, x: res + (flatten(x.iterdir()) if x.is_dir() else [str(x)]), lists, [])\n",
    "flatten(Path('.').iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_iter = lambda item: item.is_dir()\n",
    "rfunc = lambda res, x: res + (flatten(x.iterdir()) if is_iter(x) else [str(x)])\n",
    "def flatten(iterator):\n",
    "    return reduce(rfunc, iterator, [])\n",
    "flatten(Path('.').iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "is_iter = lambda item: item.is_dir()\n",
    "get_kids = lambda parent: parent.iterdir()\n",
    "get_val = lambda item: flatten(get_kids(item)) if is_iter(item) else [item]\n",
    "def flatten(iterator):\n",
    "    results = []\n",
    "    for i in iterator:\n",
    "        results = partial(operator.add, results)(get_val(i))\n",
    "    return results\n",
    "[*flatten(Path('.').iterdir())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dir(d):\n",
    "    path = Path(d)\n",
    "    if path.is_dir():\n",
    "        return [str(_) for _ in path.iterdir()]\n",
    "get_dir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "def multiplex(n, q, **kwargs):\n",
    "    \"\"\" Convert one queue into several equivalent Queues\n",
    "\n",
    "    >>> q1, q2, q3 = multiplex(3, in_q)\n",
    "    \"\"\"\n",
    "    out_queues = [Queue(**kwargs) for i in range(n)]\n",
    "    def f():\n",
    "        while True:\n",
    "            x = q.get()\n",
    "            for out_q in out_queues:\n",
    "                out_q.put(x)\n",
    "    t = Thread(target=f)\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "    return out_queues\n",
    "\n",
    "def push(in_q, out_q):\n",
    "    while True:\n",
    "        x = in_q.get()\n",
    "        out_q.put(x)\n",
    "\n",
    "def merge(*in_qs, **kwargs):\n",
    "    \"\"\" Merge multiple queues together\n",
    "\n",
    "    >>> out_q = merge(q1, q2, q3)\n",
    "    \"\"\"\n",
    "    out_q = Queue(**kwargs)\n",
    "    threads = [Thread(target=push, args=(q, out_q)) for q in in_qs]\n",
    "    for t in threads:\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "    return out_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "- Progress display\n",
    " - Create additional copy q, thread func to count items in q\n",
    " - Terminate main loop and threads upon completion rather than count down/time out\n",
    " - itertoolz.count glob?\n",
    "- Progress persistence, resumuption\n",
    "- Result persistence, db storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "from threading import Thread\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Queue()\n",
    "remote_q = client.scatter(q)\n",
    "q1, q2 = multiplex(2, remote_q)\n",
    "list_q = client.map(get_dir, q1)\n",
    "l_q = client.gather(list_q)\n",
    "\n",
    "opt_md5 = True\n",
    "\n",
    "pstat = partial(get_stat, opt_md5=opt_md5, opt_pid=False)\n",
    "q3 = client.map(pstat, q2)\n",
    "q4, q5 = multiplex(2, q3)\n",
    "result_q = client.gather(q4)\n",
    "\n",
    "qs = [q, remote_q, q1, q2, list_q, l_q, q3, q4, result_q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dir(from_q, to_q, stop=False):\n",
    "    limit = 300\n",
    "    i = limit\n",
    "    while True and ((i and not stop()) or from_q.qsize()):\n",
    "        if from_q.qsize():\n",
    "            l = from_q.get()\n",
    "            if isinstance(l, list):\n",
    "                for item in l:\n",
    "                    to_q.put(item)\n",
    "            i = min(i+1, limit)\n",
    "        else:\n",
    "            i -= 1\n",
    "            sleep(.1)\n",
    "    if stop():\n",
    "        print('load_dir stopped.')\n",
    "    elif not i:\n",
    "        print('load_dir stopped by i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unloadq(q, stop, limit=2000, rest=.1, check=100):\n",
    "    i = limit\n",
    "    loops = 0\n",
    "    results = []\n",
    "    while True and ((i and not stop()) or q.qsize()):\n",
    "        loops += 1\n",
    "        if loops % check == 0:\n",
    "            print(i, loops, len(results))\n",
    "        if q.qsize():\n",
    "            x = q.get()\n",
    "            #print(x)\n",
    "            results.append(x)\n",
    "            i = min(i+1, limit)\n",
    "        else:\n",
    "            i -= 1\n",
    "            if i % check == 0:\n",
    "                print(i)\n",
    "            sleep(rest)\n",
    "    if stop():\n",
    "        print('Unloadq stopped.')\n",
    "    elif not i:\n",
    "        print('Unloadq stopped by i')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting problem with thread not running\n",
    "\n",
    "#from concurrent.futures import ThreadPoolExecutor\n",
    "stop_threads = False\n",
    "stop = lambda: stop_threads\n",
    "basepaths = ['.']\n",
    "#t = ThreadPoolExecutor()\n",
    "thread = Thread(target=load_dir, args=(l_q, q, stop,), daemon = True)\n",
    "thread.start()\n",
    "i = 100\n",
    "while (True or l_q.qsize()) and i:\n",
    "    item = l_q.get()\n",
    "    if item:\n",
    "        i = min(i+1, 100)\n",
    "    print(item)\n",
    "    i = max(i-1, 0)\n",
    "    if i % 10 == 0:\n",
    "        print(l_q.qsize(), i)\n",
    "    sleep(.1)\n",
    "stop_threads = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_thread = Thread(target=load_dir, args=(l_q, q,), daemon = True)\n",
    "#load_thread.start()\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "basepaths = ['.']\n",
    "with ThreadPoolExecutor() as t:\n",
    "    stop_threads = False\n",
    "    stop = lambda: stop_threads\n",
    "    t_load_dir = t.submit(load_dir, l_q, q, stop)\n",
    "    print(t_load_dir.running())\n",
    "    [q.put(str(Path(path).resolve())) for path in basepaths]\n",
    "    #l_q.put(basepaths)\n",
    "    results_future = t.submit(unloadq, result_q, stop, limit=300)\n",
    "    ilimit = 10\n",
    "    i = ilimit\n",
    "    while True and i or alive:\n",
    "        alive = sum([_q.qsize() for _q in qs])\n",
    "        if alive:\n",
    "            i = min(i+1, ilimit)\n",
    "            print(alive, i)\n",
    "            print(t_load_dir.running())\n",
    "        else:\n",
    "            i -= 1\n",
    "            print(f'i: {i}')\n",
    "        sleep(.1)\n",
    "    stop_threads = True\n",
    "    #results_list = unloadq(result_q, limit=300)\n",
    "    results_list = results_future.result()\n",
    "    results = pd.DataFrame(results_list)\n",
    "    print(results.info())\n",
    "#t.shutdown(False)\n",
    "#del(load_thread)\n",
    "print(q5.qsize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[_q.qsize() for _q in qs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempting to utilize Dask df to handle, export data\n",
    "\n",
    "def iterq(q):\n",
    "    while q.qsize():\n",
    "        yield q.get()\n",
    "\n",
    "print(q5.qsize())\n",
    "result_count = q5.qsize()\n",
    "data = client.gather(q5)\n",
    "while data.qsize() < result_count:\n",
    "    print('sleeping')\n",
    "    sleep(.1)\n",
    "print(data.qsize())\n",
    "iterdata = [*iterq(data)]\n",
    "print(len(iterdata))\n",
    "df = pd.DataFrame(iterdata)\n",
    "print(len(df))\n",
    "ddf = dd.from_pandas(df, npartitions=4)\n",
    "remote_ddf = client.scatter(ddf)\n",
    "remote_result = remote_ddf.result()\n",
    "remote_result.to_csv('./export4-*.csv')\n",
    "new_ddf = dd.read_csv('./export4-*.csv')\n",
    "new_ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from toolz import itertoolz\n",
    "print(itertoolz.count(Path('../..').rglob('*')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = ['f_ctime', 'f_mtime', 'f_atime']\n",
    "results.index.name = 'id'\n",
    "results['id'] = results.index\n",
    "cols = list(set(results.columns) - set(times))\n",
    "print(len(cols))\n",
    "print(cols)\n",
    "stacked = pd.melt(results, id_vars=cols, value_vars=times, value_name='time').set_index('time')\n",
    "print(stacked.columns)\n",
    "unstacked = stacked.loc[:, ['variable']].groupby('variable').resample('D').count().unstack('variable')\n",
    "unstacked.plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir('..'))\n",
    "os.listdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_listdir(src):\n",
    "    names = os.listdir(src)\n",
    "    results = []\n",
    "    for name in names:\n",
    "        srcname = os.path.join(src, name)\n",
    "        if os.path.isdir(srcname):\n",
    "            results.extend(r_listdir(srcname))\n",
    "        else:\n",
    "            results.append(srcname)\n",
    "    return results\n",
    "len(r_listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([*os.scandir('..')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_scandir(src):\n",
    "    names = os.scandir(src)\n",
    "    results = []\n",
    "    for name in names:\n",
    "        if os.path.isdir(name):\n",
    "            results += r_scandir(name)\n",
    "        else:\n",
    "            results.append(name.path)\n",
    "    return results\n",
    "len(r_scandir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = glob.glob('**/?*', recursive=True)\n",
    "len(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "\n",
    "def r_walk(mypath):\n",
    "    f = []\n",
    "    for (dirpath, dirnames, filenames) in walk(mypath):\n",
    "        f.extend([os.path.join(dirpath, name) for name in filenames])\n",
    "    return f\n",
    "results = r_walk('.')\n",
    "len(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_path(path):\n",
    "    return [str(_) for _ in Path(path).rglob('*') if not _.is_dir()]\n",
    "len(r_path('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/OneDrive/Documents/projects/bookmarks/'\n",
    "%timeit len(r_listdir(path))\n",
    "%timeit len(r_scandir(path))\n",
    "%timeit len(r_walk(path))\n",
    "%timeit len(r_path(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../..'\n",
    "rw_set = set(r_walk(path))\n",
    "len(rw_set)\n",
    "rp_set = set(r_path(path))\n",
    "len(rp_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = r_walk(path)\n",
    "len(rw)\n",
    "rw_set = set(rw)\n",
    "len(rw_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = r_path(path)\n",
    "len(rp)\n",
    "rp_set = set(rp)\n",
    "len(rp_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time len(set(r_listdir(path)))\n",
    "%time len(set(r_scandir(path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = r_listdir(path)\n",
    "rl_set = set(rl)\n",
    "len(rl_set)\n",
    "rs = r_scandir(path)\n",
    "rs_set = set(rs)\n",
    "len(rs_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw_set - rp_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = '.'\n",
    "_, _, filenames = next(walk(mypath), (None, None, []))\n",
    "len(filenames)\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, _, filenames) = next(os.walk(mypath))\n",
    "len(filenames)\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
